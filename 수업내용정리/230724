#서버 관리 프로그램

하나의 물리서버, 하나의 서비스 > 

쿠버네티스 안에 자체적으로 로드밸랜서 기능이 있음

master-node > controller
> 모든 컨테이너들을 제어가능
node에 컨테이너가 배치되서 사용됨 / 중앙제어 시스템 구성(쿠버네티스 구성)
각각의 work node는 네트워크가 각각 다 연결이 되어 있어야하고, node안에 컨테이너 안에도 
서로 통신이 될 수 있도록 네트워크가 구성이 되어야함

etcd(DB)는 기록

스왑을 비활성화 시켜야함
스왑 : 기존에 메모리가 부족할때 HW를 직접적 메모리로 사용하는 기능
스왑 메모리가 0으로 되어있으면 사용안함

모든 서버의 /etc/hosts 파일 수정
모든 서버에 컨트롤러 및 워커 노드의 IP 주소와 hostname 정보를 등록
 >> 컨테이너끼리의 통신을 위해서 필수로 해야함

sudo kubeadm init --apiserver-advertise-address 192.168.10.11 \
 --pod-network-cidr 172.30.0.0/16 \
 --control-plane-endpoint <kube-controller>

 sed -i 's/cidr: 192\.168\.0\.0\/16/cidr: 172\.30\.0\.0\/16/g' custo-resources.yaml
kubectl create -f custom-resources.yaml
> 칼리코한테도 알려줘야함
 칼리코가 쓰는 주소 그래도 쓰면 굳이 알알려줘도 됨 기본셋팅이 192.168.0.0.d으로 설정되어있음

 230725
# token 분실시

token을 지우고 다시 
sudo kubeadm token delete token명
 새로 발급
sudo kubeadm token create --print-join-commad 

wget https://raw.githubusercontent.com/projectcalico/calico/v3.26.1/\
manifests/custom-resources.yaml
docker image pull 시 횟수제한 걸릴수 있어서. calico 실행이 안될 수도 있음

 (컨테이너를 배포하기 위한) 가장 작은 베포 단위 : pod
 -고유한 ip가 부여됨 ip는 전체 클러스터에서 고유한 ip 할당받아서 사용
 -pod안에 컨테이서 2개를 배치해서 사용 가능함
 1)host 폴더 공유(볼륨)
 2)네트워크 공유(localhost)
  >port 번호 하나를 공유해서 사용함 pod에 부여됨

# pod 생성
 1) kubectl CLI >> docker랑 비슷하게 되어있긴 함 : 직접명령어 
 >> 관리가 불편해짐, 여러개의 pod 생성, 추가적인 resources 작업이 힘듦
 2) yaml : 필요한 자원, 속성 설정해서 씀
 공유되는 dir 활용하면 편함

 visual sudio code 에서 kubenates 설치하면 편하게 yaml 작성 가능
 metadeta name
 labels 은 더 만들 수 있음
 관리의 편의성을 높이기 위해 라벨을 붙이기 때문에 관리목적, 용도에 맞게
 라벨 사용하면 됨 
 관리목적: 하나의 pod만 관리하는게 아니라, 많은양의 pod 관리할수 있도록 라벨설정
 삭제 수정시 하나씩 찾아볼 수 없으니, 공통라벨 찾아서 수정,삭제하는 경우가 있음

 spec 자원을 배치함
   containers:
  - name: myapp
    image: nginx:latest
    resources:
      limits:
        memory: "128Mi"  
        cpu: "500m" > 1 core =  1000m 으로 기록하면 됨
                    0.5 core = 500m으로 가능함           
    ports:
      - containerPort: <Port>
       > 지정한 컨테이너에서 사용할 포트번호 지정 / 사용함
       expose랑 같다고 보면됨

       kubectl apply -f /vagrant/kube_yaml/01_single_pod.yaml 실행
       pod 안에 컨테이너 만들어짐
       kubectl delete -f /vagrant/kube_yaml/01_single_pod.yaml

    kubectl create secret docker-registry docker-pull-secret \

# docker login(image pull)
--docker-server=https://index.docker.io/v1/ \
--docker-username=dy14000 \
--docker-password=dckr_pat_RnP1ykPi8-AGiDlbAa28-zcvn8k

pod yaml 파일에 추가 하기
  imagePullSecrets:
  - name: docker-pull-secret

vagrant@kube-controller:~$ kubectl apply -f /vagrant/kube_yaml/01_single_pod.yaml
pod/myapp unchanged > 변경된게 없음
pod/pod-name created > 추가한것만 새로 생성됨

vagrant@kube-controller:~$ kubectl apply -f /vagrant/kube_yaml/01_single_pod.yaml
pod/myapp unchanged
pod/pod-name configured
내용바꾸고 다시 apply하면 이렇게 나옴

vagrant@kube-controller:~$ kubectl delete -n default pod -l creater=dayoung
pod "pod-name" deleted

namespace : -n 옵션으로 지정
label : -l 옵션으로 지정해서 선택적으로 지울수있음

pod 컨테이너 : 웹으로 2개주면, 충돌~ 대부분 1개

vagrant@kube-controller:~$ kubectl exec -it web-pod-nginx -c alpine -- sh
/ # apk update > sh로 들어가서 


# ReplicaSet
 신규 생성된 pod 

apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: replicaset
spec:
  replicas: 3
  selector: #replicas 수에 해당되는 파드가 생성되었는지 확인할때
    matchLabels: # matchLabels에 작성한 라벨 정보와 일치하는 파드의 수를 확인
      app: myapp 
  template:
    metadata:
      name: replica-template
      labels:
        app: myapp #여기 라벨이 일치해야합니다
    spec:
      containers:
        - name: nginx
          image: nginx:latest
          ports:
            - containerPort: 80
      imagePullSecrets:
       - name: docker-pull-secret

# Deployment : 배포 버전관리
버전으로 다양한 pod들을 관리가능함

  template:
    metadata:
      labels:
        app: web
        type: deploy
        version: "v1.2" > 기존파일에서 수정하면,
 vagrant@kube-controller:~$ kubectl describe deployment deploy 확인하면
        Annotations:            deployment.kubernetes.io/revision: 1
        Annotations:            deployment.kubernetes.io/revision: 2


vagrant@kube-controller:~$ kubectl rollout history deployment deploy
deployment.apps/deploy 
REVISION  CHANGE-CAUSE
1         <none>
2         <none>

vagrant@kube-controller:~$ kubectl rollout history deployment deploy --revision=1
deployment.apps/deploy with revision #1
Pod Template:
  Labels:       app=web
        pod-template-hash=769ff4996f
        type=deploy
  Containers:
   nginx:
    Image:      nginx:latest
    Port:       80/TCP
    Host Port:  0/TCP
    Limits:
      cpu:      500m
      memory:   128Mi
    Environment:        <none>
    Mounts:     <none>
  Volumes:      <none>

vagrant@kube-controller:~$ kubectl rollout history deployment deploy --revision=2
deployment.apps/deploy with revision #2
Pod Template:
  Labels:       app=web
        pod-template-hash=5f95866c4d
        type=deploy
        version=v1.2
  Containers:
   nginx:
    Image:      nginx:latest
    Port:       80/TCP
    Host Port:  0/TCP
    Limits:
      cpu:      500m
      memory:   128Mi
    Environment:        <none>
    Mounts:     <none>
  Volumes:      <none>

  vagrant@kube-controller:~$ kubectl rollout undo deployment deploy --to-revision=1
deployment.apps/deploy rolled back

https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.26/#pod-v1-core
찾아볼수 있음